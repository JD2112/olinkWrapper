{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tutorial: ShinyOlink","text":"<p>ShinyOlink is a user-friendly Shiny application designed to simplify the analysis of Olink proteomics data. It allows users to upload their raw data, then performs various statistical tests and generates interactive visualizations by leveraging the specialized <code>OlinkAnalyze</code> R package, making complex bioinformatics accessible to researchers. The project uses a modular design and has automated deployment pipelines for efficient delivery and maintenance.</p>"},{"location":"#visual-overview","title":"Visual Overview","text":"<pre><code>flowchart TD\n    A0[\"Shiny Application Structure\"]\n    A1[\"Module-Based Development\"]\n    A2[\"Data Ingestion &amp; Preparation\"]\n    A3[\"OlinkAnalyze Package Integration\"]\n    A4[\"Reactive Programming\"]\n    A5[\"Statistical Analysis &amp; Visualization Modules\"]\n    A6[\"Automated Deployment Pipelines\"]\n    A7[\"Project Governance &amp; Contributions\"]\n    A0 -- \"Composed of Modules\" --&gt; A1\n    A0 -- \"Underpins Application Logic\" --&gt; A4\n    A1 -- \"Implements Functionality\" --&gt; A5\n    A1 -- \"Manages Data Upload\" --&gt; A2\n    A2 -- \"Provides Reactive Data\" --&gt; A4\n    A2 -- \"Supplies Data For\" --&gt; A5\n    A3 -- \"Provides Analysis Functions\" --&gt; A5\n    A4 -- \"Enables Dynamic Outputs\" --&gt; A5\n    A6 -- \"Automates Deployment\" --&gt; A0\n    A7 -- \"Governs Deployment\" --&gt; A6\n    A7 -- \"Guides Module Design\" --&gt; A1</code></pre>"},{"location":"#chapters","title":"Chapters","text":"<ol> <li>Manual</li> <li>Olink Proteomics</li> <li>Shiny,R and Docker</li> <li>FAQs</li> </ol>"},{"location":"chapter1/","title":"OlinkWrapper App Documentation","text":"<p>Welcome to the documentation for the Olink Wrapper App. This guide is designed to help you understand how to use the app, the scientific principles behind the data, and the technical stack that powers it.</p> Note to Users <p>For each analysis, a progress bar will appear to indicate that calculations are in progress.</p> <ul> <li>If any errors occur during analysis, an error message will be displayed.</li> <li>Make sure your data is properly formatted before uploading:<ul> <li>NPX Data should contain columns for <code>SampleID</code>, <code>Assay</code>, and <code>NPX</code> values.</li> <li>Key File should map <code>SampleID</code> to <code>SUBJID</code> (Subject Identifier).</li> <li>Variables File should contain additional variables for each <code>SUBJID</code> (Subject Identifier).</li> </ul> </li> </ul> <p>This app provides a user-friendly interface for comprehensive analysis of Olink\u2122 data. If you encounter any issues or have questions about the results, please consult with a statistician or a bioinformatics expert for proper interpretation.</p>"},{"location":"chapter1/#1-app-user-manual-a-step-by-step-guide","title":"1. App User Manual: A Step-by-Step Guide","text":"<p>The Olink Wrapper App is a user-friendly tool for the analysis and visualization of Olink proteomics data. The app's interface is divided into several panels that guide you through a typical data analysis workflow, from uploading your data to generating publication-ready plots.</p>"},{"location":"chapter1/#11-getting-started-data-input","title":"1.1 Getting Started: Data Input","text":"<p>The app requires specific data files to begin analysis.</p> <ul> <li>Upload NPX Data (CSV): This is the main data file containing your Normalized Protein Expression (NPX) values. It should have columns for SampleID, Assay, and NPX.</li> <li>Upload Key File (CSV, optional): A file that contains information to match your SampleID to metadata from a different source. This is useful if your primary NPX data lacks sufficient sample metadata.</li> <li>Upload Variables File (CSV): This file contains all your sample-level metadata, such as SubjectID, Treatment, TimePoint, or any other experimental variables you want to use for analysis and plotting.</li> </ul> Demo Data Download <p>Please download the demo data from the following link to practice with the olinkWrapper app: https://sourceforge.net/projects/olinkwrapper/</p> <p>Remember: download all three files.</p> <p>After uploading your files, click \"Merge Data\" to combine them into a single dataset for analysis. You can preview the merged data in the A. Data Preview tab.</p> <p></p>"},{"location":"chapter1/#b-preprocessing","title":"B. Preprocessing","text":"<p>This section allows you to clean and prepare your data for statistical analysis.</p>"},{"location":"chapter1/#b1-bridge-selector","title":"B.1 Bridge Selector:","text":"<p>This step is crucial for quality control when you have data from multiple Olink panels. It allows you to select which \"bridge\" protein data to use, ensuring consistency across different runs.</p>"},{"location":"chapter1/#b2-normalization","title":"B.2 Normalization:","text":"<p>This function applies methods to adjust NPX values, helping to reduce technical variation and make the data more comparable across samples.</p>"},{"location":"chapter1/#b3-lod-limit-of-detection","title":"B.3 LOD (Limit of Detection):","text":"<p>Here, you can filter out proteins with NPX values below the assay's limit of detection. This ensures that you only analyze reliable measurements, improving the quality of your results.</p>"},{"location":"chapter1/#b4-outlier-detection","title":"B.4 Outlier Detection:","text":"<p>This tool helps you identify and optionally remove samples or assays that are statistically unusual, which may be due to experimental errors.</p> <p></p>"},{"location":"chapter1/#c-statistical-analysis-fundamentals-manual","title":"C. Statistical Analysis: Fundamentals &amp; Manual","text":"<p>The app provides several common statistical tests to find meaningful differences in your data.</p>"},{"location":"chapter1/#c1-normality-test","title":"C.1 Normality Test:","text":"<p>This test determines if your data follows a normal (bell-shaped) distribution. Many parametric tests, like the T-test and ANOVA, assume normality.</p> <p>How to run: Select your variables and click the \"Normality Test\" button to check the distribution of your data.</p> <p></p>"},{"location":"chapter1/#c2-t-test","title":"C.2 T-test:","text":"<p>A T-test is used to determine if there is a significant difference between the means of two groups.</p> <p>How to run: </p> <ul> <li>Navigate to the \"T-Test\" tab.</li> <li>Select a grouping variable and specify its type (Character or Factor).</li> <li>Click \"Run T-Test\" to perform the analysis.</li> <li>Results will be displayed in a table.</li> <li>Use the \"Download Results\" button to save the T-Test results as an Excel file.</li> </ul> <p></p>"},{"location":"chapter1/#c3-wilcoxon-test","title":"C.3 Wilcoxon Test:","text":"<p>This is a non-parametric alternative to the T-test. It is used when your data is not normally distributed or when you have ordinal data. It checks if there is a significant difference between two groups without assuming a specific distribution.</p> <p>How to run: Select a grouping variable and click \"Wilcoxon Test.\"</p> <p></p>"},{"location":"chapter1/#c4-anova-analysis-of-variance","title":"C.4 ANOVA (Analysis of Variance):","text":"<p>ANOVA is used to compare the means of three or more groups to see if at least one group mean is different from the others.</p> <p>How to run:</p> <ul> <li>In the \"ANOVA\" tab, select a grouping variable and its type.</li> <li>Choose the number of covariates (0-4) if needed.</li> <li>If covariates are selected, choose the covariate variables from the dropdown menus.</li> <li>Click \"Run ANOVA\" to perform the analysis.</li> <li>Results will be displayed in a table.</li> <li>Use the \"Download Results\" button to save the ANOVA results as an Excel file.</li> </ul> <p></p>"},{"location":"chapter1/#c5-anova-post-hoc","title":"C.5 ANOVA Post-hoc:","text":"<p>If an ANOVA test is significant, a post-hoc test is used to determine which specific groups are different from each other.</p> <p>How to run: After running ANOVA, select the groups you want to compare and run the post-hoc test.</p> <p>ANOVA Post-hoc</p> <p>Please ensure you have run the ANOVA test before attempting to run the ANOVA Post-hoc test, as it relies on the results from that analysis.</p> <p></p>"},{"location":"chapter1/#c6-linear-mixed-effects-lme-and-post-hoc","title":"C.6 Linear Mixed Effects (LME) and Post-hoc:","text":"<p>LME models are powerful tools used for analyzing data where samples are not independent, such as longitudinal studies where the same subjects are measured at different time points.</p> <p>How to run: Define both fixed effects (e.g., Treatment) and random effects (e.g., SubjectID) in the app's interface and click \"LME.\" Use the post-hoc function to find specific differences.</p>"},{"location":"chapter1/#d-exploratory-analysis-finding-patterns","title":"D. Exploratory Analysis: Finding Patterns","text":"<p>Exploratory analysis helps you visualize the overall structure of your data and find patterns without making specific assumptions.</p>"},{"location":"chapter1/#d1-pca-plot-principal-component-analysis","title":"D.1 PCA Plot (Principal Component Analysis):","text":"<p>PCA is a method that reduces the dimensionality of your data, allowing you to visualize hundreds of proteins in a simple 2D or 3D plot. Samples that are similar in protein expression will cluster together.</p> <p>How to run: Select the variables to color or shape your plot and click \"PCA.\"</p> <p></p>"},{"location":"chapter1/#d2-umap-plot-uniform-manifold-approximation-and-projection","title":"D.2 UMAP Plot (Uniform Manifold Approximation and Projection):","text":"<p>Similar to PCA, UMAP is a non-linear dimensionality reduction technique that is often more effective at revealing complex relationships and clusters in your data.</p> <p>How to run: Select your grouping variables and click \"UMAP.\"</p> <p></p>"},{"location":"chapter1/#e-visualization-creating-plots","title":"E. Visualization: Creating Plots","text":"<p>The app generates a variety of plots to help you understand and present your results.</p>"},{"location":"chapter1/#e1-box-plot","title":"E.1 Box Plot:","text":"<p>A box plot provides a quick visual summary of the distribution of NPX values for a single protein across different groups, showing the median, quartiles, and outliers.</p> <p></p>"},{"location":"chapter1/#e2-distribution-plot","title":"E.2. Distribution Plot:","text":"<p>This plot, often a histogram or density plot, shows the frequency distribution of NPX values for a given protein.</p>"},{"location":"chapter1/#e3-lme-plot","title":"E.3 LME Plot:","text":"<p>This plot visualizes the results of a Linear Mixed Effects model, often showing the protein expression change over time or across different conditions, accounting for within-subject variability.</p>"},{"location":"chapter1/#e4-pathway-heatmap","title":"E.4 Pathway Heatmap:","text":"<p>A heatmap is a color-coded matrix that shows the expression levels of proteins involved in a specific biological pathway. It requires you to first run a Pathway Enrichment Analysis.</p> <p>Pathway Heatmap</p> <p>Please ensure you have run the Pathway Enrichment Analysis before attempting to create a Pathway Heatmap, as it relies on the results from that analysis.</p> <p></p>"},{"location":"chapter1/#e5-qc-plot-quality-control","title":"E.5 QC Plot (Quality Control):","text":"<p>This plot helps you assess the quality of your data, often by showing the overall NPX distribution across all samples or assays.</p> <p></p>"},{"location":"chapter1/#e6-heatmap-plot","title":"E.6 Heatmap Plot:","text":"<p>A general heatmap that displays the expression of many proteins across many samples, allowing you to see global patterns and clusters of co-expressed proteins.</p>"},{"location":"chapter1/#e7-volcano-plot","title":"E.7 Volcano Plot:","text":"<p>A key visualization for T-test or Wilcoxon results. It plots the statistical significance (p-value) against the magnitude of change (fold change or effect size) for all proteins, making it easy to spot significant differences.</p> <ul> <li>Go to the \"Volcano Plot\" tab.</li> <li>Select a grouping variable and its type.</li> <li>Click \"Generate Volcano Plot\" to create the plot.</li> <li>Use the \"Download Plot\" button to save the Volcano plot.</li> </ul> <p>Volcano Plot</p> <p>Please ensure you have run either the T-test or Wilcoxon test before attempting to create a Volcano Plot, as it relies on the results from those analyses.</p> <p></p>"},{"location":"chapter1/#e8-violin-plot","title":"E.8 Violin Plot:","text":"<p>Similar to a box plot, but it also shows the density of the data at different NPX values, providing a more detailed look at the data distribution.</p> <ul> <li>In the \"Violin Plot\" tab, select a protein and a grouping variable.</li> <li>Specify the variable type for the grouping variable.</li> <li>Click \"Generate Violin Plot\" to create the plot.</li> <li>Use the \"Download Plot\" button to save the Violin plot.</li> </ul> <p></p>"},{"location":"chapter1/#f-pathway-enrichment-analysis","title":"F. Pathway Enrichment Analysis","text":"<p>This is a powerful method to go beyond individual protein analysis. It uses your list of significantly changed proteins (identified by a T-test or Wilcoxon test) and compares them to known biological pathways. It tells you which pathways are significantly represented in your list of proteins, helping you interpret your findings in a biological context.</p> <p>Pathway Enrichment Analysis</p> <p>Please ensure you have run either the T-test or Wilcoxon test before attempting to run the Pathway Enrichment Analysis, as it relies on the results from those analyses.</p> <p></p> <p></p>"},{"location":"chapter1/#g-linear-regression","title":"G. Linear Regression","text":"<p>Linear regression is used to model the relationship between a protein's NPX value (dependent variable) and one or more other variables (independent variables), such as age, BMI, or a clinical score. This helps you identify proteins whose levels are associated with continuous variables.</p> <p></p>"},{"location":"chapter1/#references","title":"References","text":"<ol> <li>Olink Proteomics Official Website: https://www.olink.com/</li> <li>Assarsson, E. et al. (2014). \"A single-tube, quantitative technique for high-throughput protein analysis.\" Nature Methods, 11(6), 665\u2013670.</li> <li>OlinkAnalyze R Package: https://cran.r-project.org/web/packages/OlinkAnalyze/refman/OlinkAnalyze.html</li> <li>OlinkAnalyze Vigenette: https://cran.r-project.org/web/packages/OlinkAnalyze/vignettes/Vignett.html </li> </ol>"},{"location":"chapter2/","title":"2. Background Theory: Olink Proteomics","text":"<p>This section provides a brief overview of the technology that generates the data analyzed by the app.</p>"},{"location":"chapter2/#21-what-is-proteomics","title":"2.1 What is Proteomics?","text":"<p>Proteomics is the large-scale study of proteins. It's a fundamental field of study in biology that aims to understand the structure, function, and interactions of proteins, as well as how they change in response to different conditions. Proteins are the workhorses of the cell, and their levels and activity provide a dynamic snapshot of biological processes.</p>"},{"location":"chapter2/#22-proximity-extension-assay-pea","title":"2.2 Proximity Extension Assay (PEA)","text":"<p>The Olink platform is based on the Proximity Extension Assay (PEA) technology. This highly sensitive and specific method enables the simultaneous measurement of hundreds of proteins from a very small sample volume (typically 1-3 microliters of plasma, serum, or other biological fluid). The core principle is as follows:</p> <ol> <li>Antibody Pairs: Each protein of interest is targeted by a pair of antibodies, each labeled with a unique DNA oligonucleotide.</li> <li>Proximity: If both antibodies successfully bind to the same target protein, their attached DNA oligos are brought into close proximity.</li> <li>Hybridization and Extension: The close proximity allows the oligos to hybridize and serve as a template for a DNA polymerase. The polymerase extends the hybridized strands, creating a new, unique DNA sequence (a \"reporter sequence\") for that specific protein.</li> <li>Quantification: The newly formed reporter sequences are then quantified using high-throughput real-time PCR (qPCR), providing a readout proportional to the amount of the target protein in the original sample. This value is reported as Normalized Protein Expression (NPX), which is on a log2 scale.</li> </ol> <p>The PEA technology's high specificity, as it requires two antibodies to bind to the same protein, minimizes false positive signals and allows for robust, multiplexed protein analysis.</p>"},{"location":"chapter2/#references","title":"References","text":"<p>Assarsson, E. et al. (2014). \"A single-tube, quantitative technique for high-throughput protein analysis.\" Nature Methods, 11(6), 665\u2013670.</p> <p>Olink Proteomics Official Website: https://www.olink.com/</p>"},{"location":"chapter3/","title":"3. A Note on the App's Technical Stack","text":"<p>The OlinkWrapper App is built with a powerful combination of open-source and modern technologies to ensure it is both interactive and reproducible.</p>"},{"location":"chapter3/#31-shiny","title":"3.1 Shiny","text":"<p>Shiny is an open-source R package that provides a robust framework for building interactive web applications using R. It allows researchers and data scientists to create powerful applications with a live backend powered by R's analytical capabilities, without needing extensive web development knowledge. The app's user interface, plots, and statistical computations are all handled by the Shiny framework.</p>"},{"location":"chapter3/#32-r-packages","title":"3.2 R Packages","text":"<p>The app relies on a variety of R packages to perform its functions. While the full list can be found in the DESCRIPTION file of the GitHub repository, key packages likely include:</p> <ul> <li><code>shiny</code>: The core package for the web application framework.</li> <li><code>dplyr</code>: For efficient data manipulation and cleaning.</li> <li><code>ggplot2</code>: For generating high-quality, customizable data visualizations.</li> <li><code>plotly</code>: For creating interactive plots that allow users to explore data.</li> <li><code>olink</code>: For specific functions related to Olink data handling and analysis.</li> <li><code>readxl</code>: For reading data from Excel files.</li> </ul>"},{"location":"chapter3/#33-docker","title":"3.3 Docker","text":"<p>Docker is a platform that uses containerization to package an application and all its dependencies (including the R environment, Shiny, and all required packages) into a single, isolated \"container.\" This container can be run on any system that has Docker installed. The use of Docker provides several key benefits:</p> <ul> <li>Reproducibility: The app will run exactly the same way every time, regardless of the host system's configuration.</li> <li>Simplified Deployment: Deployment is as simple as building and running the Docker image, eliminating complex dependency management issues.</li> <li>Isolation: The app runs in its own environment, preventing conflicts with other applications or system configurations.</li> </ul> <p>By using Docker, the app ensures that everyone from the developer to an end-user has a consistent and reliable experience, making the analysis and results fully reproducible.</p>"},{"location":"chapter4/","title":"Frequently Asked Questions (FAQ)","text":"What should I do if the app looks frozen during analysis? <p>A progress bar will always appear while calculations are in progress. If it stops or an error occurs, an error message will be displayed.  </p> How should my input files be formatted? <p>You need three files: - NPX Data (CSV): Must contain <code>SampleID</code>, <code>Assay</code>, and <code>NPX</code> columns. - Key File (CSV, optional): Maps <code>SampleID</code> to <code>SUBJID</code>. - Variables File (CSV): Contains additional variables (e.g., <code>SUBJID</code>, Treatment, TimePoint).  </p> Can I test the app without my own data? <p>Yes. Download the demo dataset (NPX Data, Key File, Variables File) from OlinkWrapper on SourceForge.  </p>"},{"location":"chapter4/#preprocessing","title":"Preprocessing","text":"Why do I need a Bridge Selector? <p>Bridge proteins ensure consistency across multiple Olink panels. Selecting the right bridge protein is crucial for quality control.  </p> What is normalization and why is it important? <p>Normalization adjusts NPX values to reduce technical variation, making samples comparable.  </p> How do I handle proteins below the limit of detection (LOD)? <p>Use the LOD filtering step to remove proteins with NPX values below the assay\u2019s detection limit. This ensures that only reliable protein measurements are included.  </p> How do I detect and handle outliers? <p>The Outlier Detection tool identifies unusual samples or assays. You can remove them if they compromise your analysis.  </p>"},{"location":"chapter4/#statistical-analysis","title":"Statistical Analysis","text":"Which test should I use to compare two groups? <ul> <li>Use a T-test if your data is normally distributed.  </li> <li>Use a Wilcoxon test if your data is non-normal or ordinal.  </li> </ul> What if I have more than two groups? <p>Use ANOVA to test differences across three or more groups. If significant, run a Post-hoc test to see which groups differ.  </p> What if my samples are not independent (e.g., repeated measures)? <p>Use Linear Mixed Effects (LME) models, which handle random effects like subject IDs in longitudinal studies.  </p>"},{"location":"chapter4/#visualization","title":"Visualization","text":"Why is my Volcano Plot empty? <p>You must first run a T-test or Wilcoxon test. The Volcano Plot depends on those results.  </p> Why can\u2019t I generate a Pathway Heatmap? <p>You must run a Pathway Enrichment Analysis first. The heatmap relies on enrichment results.  </p> How can I explore global protein patterns? <p>Use PCA or UMAP plots to visualize high-dimensional data in 2D/3D.  </p> What plots are available for presenting results? <p>The app supports: - Box plots - Violin plots - Heatmaps - QC plots - LME plots - Volcano plots - Pathway heatmaps - Distribution plots  </p>"},{"location":"chapter4/#advanced-analysis","title":"Advanced Analysis","text":"What is Pathway Enrichment Analysis? <p>It tests whether your significant proteins (from T-test or Wilcoxon) are overrepresented in known biological pathways.  </p> Can I run linear regression in the app? <p>Yes. Linear regression models protein NPX levels against continuous variables (e.g., age, BMI, clinical scores).  </p>"},{"location":"chapter4/#server-browser-issues","title":"Server &amp; Browser Issues","text":"I see a 404 error when accessing the app. What should I do? <ul> <li>Wait a few moments and refresh the browser.  </li> <li>If the problem persists, clear your browser cache and cookies.  </li> <li>Restart your browser or computer before retrying.  </li> <li>If you still cannot access the app, contact your system administrator.  </li> </ul> The app is very slow or not responding. How can I fix this? <ul> <li>Ensure you have a stable internet connection.  </li> <li>Try using a different browser (Google Chrome or Firefox recommended).  </li> <li>Close unnecessary browser tabs or applications consuming high memory.  </li> <li>If running locally, check that your machine has sufficient RAM/CPU for the dataset size.  </li> </ul> The plots or tables are not displaying properly. <ul> <li>Refresh the browser page.  </li> <li>Clear the browser cache and reload.  </li> <li>Try resizing the browser window or switching to full-screen mode.  </li> <li>If the problem continues, restart the app/server.  </li> </ul> The app logged me out unexpectedly. <ul> <li>This can happen if your session times out.  </li> <li>Refresh the browser and log in again.  </li> <li>If the issue repeats frequently, ask your administrator to extend the session timeout.</li> <li>It can also occur due to the ill-formatted input files. Please double-check your files with the required format (See \"Demo Data\" section).</li> </ul> I uploaded my files but nothing happens. <ul> <li>Double-check that the files are in the correct CSV format.  </li> <li>Ensure your NPX Data, Key File, and Variables File follow the required structure.  </li> <li>Re-upload the files one by one and then click Merge Data.  </li> <li>If the issue persists, try using a different browser or clear your current browser's cache.  </li> <li>Contact support if you continue to experience issues.</li> </ul>"},{"location":"chapter5/","title":"Chapter 5: Module-Based Development","text":"<p>Welcome back, <code>ShinyOlink</code> enthusiast! In Chapter 4: Shiny Application Structure, we learned that every Shiny app has a User Interface (UI) and Server logic, and how <code>app.R</code> brings them together. You also saw that <code>ShinyOlink</code> is a big app, and its UI and Server code are split into many files like <code>ui_main.R</code> and <code>server_main.R</code>.</p> <p>While splitting code into files is a good start, for very large applications like <code>ShinyOlink</code> with many features, even splitting can become messy. Imagine trying to manage dozens of separate UI and server files and making sure they all play nicely together!</p>"},{"location":"chapter5/#what-is-module-based-development","title":"What is \"Module-Based Development\"?","text":"<p>Think of building with LEGO bricks. You don't just get a giant pile of plastic and start melting it together. Instead, you get pre-made bricks of different shapes and sizes \u2013 a window brick, a door brick, a wall brick. Each brick is a self-contained unit that does one job. You can build a small house or a huge castle by simply snapping these bricks together.</p> <p>The main problem \"Module-Based Development\" solves is this: How do we organize a complex Shiny application, with many different features (like T-tests, ANOVA, PCA, etc.), so that the code is easy to understand, maintain, and reuse?</p> <p>In <code>ShinyOlink</code>, instead of putting all the code for, say, the T-test feature directly into <code>ui_main.R</code> and <code>server_main.R</code>, we package it up as a \"module.\" This module has its own UI part and its own server part, specifically designed to handle everything related to T-tests.</p> <p>Let's use our T-test feature as an example: The goal is to compare protein levels between two groups and display the results and a Volcano Plot. With modules, we treat this entire feature \u2013 its buttons, tables, plots, and the logic that makes them work \u2013 as one independent, reusable \"brick.\"</p>"},{"location":"chapter5/#key-concepts-your-reusable-lego-bricks","title":"Key Concepts: Your Reusable LEGO Bricks","text":"<p>A Shiny \"module\" is essentially a pair of functions: one for the UI and one for the server logic. They work together to create a self-contained, reusable piece of your application.</p> Concept Description Analogy Module UI Function Defines the user interface (buttons, inputs, outputs) for just that specific feature. A specialized LEGO brick, like a pre-assembled window. Module Server Function Contains the logic that reacts to inputs and generates outputs for just that specific feature. The internal mechanism that makes the LEGO window open and close. Encapsulation Each module keeps its code and internal workings separate from other modules. Each LEGO brick works independently, without interfering with others. Reusability You can use the same module multiple times in different parts of your app, or even in different apps! You can use the same window brick in many houses or castles."},{"location":"chapter5/#how-to-use-modules-in-shinyolink","title":"How to Use Modules in <code>ShinyOlink</code>","text":"<p><code>ShinyOlink</code> is built almost entirely using modules. Each major analysis or visualization feature (like ANOVA, PCA, T-Test, Volcano Plot) has its own dedicated pair of UI and server module files.</p> <p>When you interact with <code>ShinyOlink</code>:</p> <ol> <li>The main UI (<code>ui_main.R</code>) calls the UI functions of various modules to build the overall layout. For example, it calls <code>ttest_ui()</code> to put the T-Test controls on the screen.</li> <li>The main Server (<code>server_main.R</code>) calls the server functions of these modules to activate their specific logic. For example, it calls <code>ttest_server()</code> to handle what happens when you click the \"Run T-Test\" button.</li> </ol> <p>Let's revisit our T-test example. To make the T-test feature available, <code>ShinyOlink</code> does not just add elements directly to <code>ui_main.R</code> and <code>server_main.R</code>. Instead, it includes the T-test module's UI and server functions.</p>"},{"location":"chapter5/#example-the-t-test-module","title":"Example: The T-Test Module","text":"<p>Recall from Chapter 2: Statistical Analysis &amp; Visualization Modules that we have a file called <code>app/ui/ui_ttest.R</code> and another called <code>app/server/server_ttest.R</code>. These files define the T-Test module.</p> <p>Here's a simplified look at the <code>ttest_ui</code> function:</p> <p><pre><code># app/ui/ui_ttest.R (Simplified T-Test Module UI)\nttest_ui &lt;- function() {\n  tagList( # tagList helps organize multiple UI elements\n    selectInput(\"ttest_var\", \"Grouping Variable\", choices = NULL),\n    radioButtons(\"ttest_var_type\", \"Variable Type\", choices = c(\"Character\", \"Factor\")),\n    actionButton(\"run_ttest\", \"Run T-Test\", class = \"btn-primary\"),\n    DTOutput(\"ttest_output\"), # Output for the results table\n    downloadButton(\"download_ttest\", \"Download Results\")\n  )\n}\n</code></pre> Explanation: This <code>ttest_ui</code> function is the Module UI function. It defines all the dropdowns, buttons, and places where outputs (like the table) will appear, specifically for the T-test feature. When the main application wants to show the T-test section, it simply \"calls\" this function, and all these elements show up.</p> <p>And here's a simplified look at the <code>ttest_server</code> function:</p> <p><pre><code># app/server/server_ttest.R (Simplified T-Test Module Server)\nttest_server &lt;- function(input, output, session, merged_data, ttest_results) {\n  observeEvent(input$run_ttest, {\n    req(merged_data(), input$ttest_var)\n    data_for_test &lt;- merged_data()\n    # ... (code to prepare data and call olink_ttest) ...\n    results &lt;- olink_ttest(data_for_test, variable = input$ttest_var)\n    ttest_results(results) # Store results for other modules\n    output$ttest_output &lt;- renderDT({\n      datatable(results)\n    })\n  })\n  # ... (code for downloading results) ...\n}\n</code></pre> Explanation: This <code>ttest_server</code> function is the Module Server function. It contains all the \"brain\" logic for the T-test. It listens for the <code>run_ttest</code> button click, gets the <code>merged_data</code> (our input), runs the <code>olink_ttest</code> function (from Chapter 3: OlinkAnalyze Package Integration), and then updates <code>ttest_results</code> and <code>output$ttest_output</code> (our outputs). Notice how it also takes <code>merged_data</code> and <code>ttest_results</code> as inputs \u2013 this is how modules can share information!</p>"},{"location":"chapter5/#under-the-hood-plugging-in-the-lego-bricks","title":"Under the Hood: Plugging in the LEGO Bricks","text":"<p>Let's see how these <code>ttest_ui</code> and <code>ttest_server</code> \"LEGO bricks\" are plugged into the main <code>ShinyOlink</code> application.</p>"},{"location":"chapter5/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>Main UI Calls Module UI: When <code>ShinyOlink</code> starts, its main UI definition (in <code>app/ui/ui_main.R</code>) includes a line that calls the <code>ttest_ui()</code> function. This tells Shiny to draw all the T-Test specific buttons and inputs on the screen.</li> <li>Main Server Calls Module Server: Similarly, the main server logic (in <code>app/server/server_main.R</code>) includes a line that calls the <code>ttest_server()</code> function. This activates the T-Test module's brain, making it ready to listen for clicks and perform calculations.</li> <li>Data Flow: When the <code>ttest_server()</code> is called, it's given access to important shared data, like <code>merged_data</code> (our combined Olink dataset from Chapter 1: Data Ingestion &amp; Preparation) and a place to store its results, like <code>ttest_results</code>. This allows the module to work with the data and pass its outcomes to other parts of the app (like the Volcano Plot module).</li> </ol> <p>Here's a simple diagram to visualize this process:</p> <pre><code>sequenceDiagram\n    participant Shiny App (UI)\n    participant Shiny App (Server)\n    participant ui_main.R\n    participant server_main.R\n    participant T-Test UI Module\n    participant T-Test Server Module\n\n    Shiny App (UI)-&gt;&gt;ui_main.R: App starts, builds overall UI\n    ui_main.R-&gt;&gt;T-Test UI Module: Calls ttest_ui() to get T-Test controls\n    T-Test UI Module--&gt;&gt;ui_main.R: Returns T-Test UI elements\n    ui_main.R--&gt;&gt;Shiny App (UI): T-Test controls displayed\n\n    Shiny App (Server)-&gt;&gt;server_main.R: App starts, activates overall server logic\n    server_main.R-&gt;&gt;T-Test Server Module: Calls ttest_server() with shared data (merged_data, ttest_results)\n    Note over T-Test Server Module: T-Test module is now active and listening for user input\n    User-&gt;&gt;Shiny App (UI): Clicks \"Run T-Test\"\n    Shiny App (UI)-&gt;&gt;Shiny App (Server): Sends input$run_ttest signal\n    Shiny App (Server)-&gt;&gt;T-Test Server Module: Forwards signal\n    T-Test Server Module--&gt;&gt;T-Test Server Module: Performs calculations using merged_data\n    T-Test Server Module--&gt;&gt;T-Test Server Module: Stores results in ttest_results\n    T-Test Server Module--&gt;&gt;Shiny App (Server): Sends output for display\n    Shiny App (Server)--&gt;&gt;Shiny App (UI): Displays T-Test results table</code></pre>"},{"location":"chapter5/#diving-into-the-code-the-plug-in-lines","title":"Diving into the Code: The \"Plug-in\" Lines","text":"<p>The key to module-based development in <code>ShinyOlink</code> lies in <code>app/ui/ui_main.R</code> and <code>app/server/server_main.R</code>, where these modules are called.</p>"},{"location":"chapter5/#calling-module-ui-in-appuiui_mainr","title":"Calling Module UI in <code>app/ui/ui_main.R</code>","text":"<p>To include the T-Test UI in the application, <code>ui_main.R</code> first needs to know where <code>ttest_ui.R</code> is, and then it calls the function.</p> <p><pre><code># app/ui/ui_main.R (Simplified for calling T-Test UI)\n# ... other library and source calls ...\nsource(\"ui/ui_ttest.R\") # This line tells R where to find the ttest_ui function\n\nsingle_ui &lt;- function() {\n  page_sidebar(\n    # ... general app layout ...\n    navset_tab(\n      nav_panel(\"C. Statistical Analysis\",\n        navset_pill(\n          nav_panel(\"2. T-Test\", ttest_ui()) # This line *calls* the T-Test UI module!\n        )\n      )\n      # ... other nav_panels ...\n    )\n    # ... footer content ...\n  )\n}\n</code></pre> Explanation: The <code>source(\"ui/ui_ttest.R\")</code> line loads the code from the <code>ui_ttest.R</code> file, making the <code>ttest_ui</code> function available. Then, <code>nav_panel(\"2. T-Test\", ttest_ui())</code> is where the <code>ttest_ui</code> function is called. When <code>ShinyOlink</code> builds its interface, it executes <code>ttest_ui()</code>, and all the UI elements defined in that module function are added to the \"T-Test\" tab.</p>"},{"location":"chapter5/#calling-module-server-in-appserverserver_mainr","title":"Calling Module Server in <code>app/server/server_main.R</code>","text":"<p>The server side works similarly. The <code>server_main.R</code> file first sources all the individual server module files and then calls their functions.</p> <p><pre><code># app/server/server_main.R (Simplified for calling T-Test Server)\n# ... many lines sourcing other server module files ...\nsource(file.path(\"server\", \"server_ttest.R\")) # This line loads the ttest_server function\n# ... more source calls ...\n\nserver_main &lt;- function(input, output, session, merged_data, var_key_merged, ttest_results) {\n  # ... other reactive values ...\n\n  # This line *calls* the T-Test Server module!\n  safe_call(ttest_server, input, output, session, merged_data, ttest_results)\n\n  # ... calls to many more server modules ...\n}\n</code></pre> Explanation: *   <code>source(file.path(\"server\", \"server_ttest.R\"))</code> loads the <code>ttest_server</code> function. *   <code>safe_call(ttest_server, input, output, session, merged_data, ttest_results)</code> is the crucial line. This calls the <code>ttest_server</code> function. It passes <code>input</code>, <code>output</code>, and <code>session</code> (standard Shiny variables) along with <code>merged_data</code> (the main dataset shared across the app) and <code>ttest_results</code> (a reactive value where the T-test results will be stored and can be accessed by other modules, like the Volcano Plot). The <code>safe_call</code> function is a helper used in <code>ShinyOlink</code> to ensure robustness, but conceptually, it's just calling <code>ttest_server(...)</code>.</p> <p>This modular approach makes <code>ShinyOlink</code> much more manageable. If there's an issue with the T-test calculations, developers know exactly which files (<code>ui_ttest.R</code> and <code>server_ttest.R</code>) to look at, without affecting other parts of the application. It also makes it easy to add new features later \u2013 just create a new module pair and plug it in!</p>"},{"location":"chapter5/#conclusion","title":"Conclusion","text":"<p>In this chapter, you've learned about the power of Module-Based Development in <code>ShinyOlink</code>:</p> <ul> <li>It helps organize complex Shiny applications by breaking them into smaller, self-contained \"LEGO bricks\" (UI function + Server function pairs).</li> <li>Each module is responsible for a specific feature, making the code easier to understand, maintain, and reuse.</li> <li>You saw how <code>ShinyOlink</code> uses this approach, with <code>ui_main.R</code> calling module UI functions and <code>server_main.R</code> activating module server functions, passing shared data between them.</li> </ul> <p>This modular structure is a cornerstone of building robust and scalable Shiny applications. It also sets the stage for understanding how data flows and changes within the app, which is handled by a concept called \"Reactive Programming.\"</p> <p>Ready to understand how these modules dynamically respond to your actions? Let's move on to Chapter 6: Reactive Programming!</p>"},{"location":"chapter6/","title":"Chapter 6: Reactive Programming","text":"<p>Welcome back, <code>ShinyOlink</code> enthusiast! In Chapter 5: Module-Based Development, we learned how <code>ShinyOlink</code> organizes its code into smaller, reusable \"LEGO bricks\" called modules. This modular design helps keep the code tidy and makes the application easier to manage.</p> <p>But how do these individual \"bricks\" communicate with each other? How does a change you make in one part of the app (like uploading data) automatically affect another part (like updating the list of variables you can analyze)? This is where Reactive Programming comes in!</p>"},{"location":"chapter6/#what-is-reactive-programming","title":"What is \"Reactive Programming\"?","text":"<p>Imagine you're working with a spreadsheet. When you change a number in one cell, all the formulas in other cells that depend on that number automatically recalculate and update their results. You don't have to tell each formula to re-run; they just react to the change.</p> <p>The main problem Reactive Programming solves is this: How do we make a web application feel \"alive\" and responsive, so that when a user does something (like clicking a button or typing text), other parts of the application automatically update without the user having to refresh the page or manually trigger every single change?</p> <p>In <code>ShinyOlink</code>, reactive programming is the magic that allows the app to respond dynamically to your actions. For example:</p> <ul> <li>When you upload your data and click \"Merge Data\" (from Chapter 1: Data Ingestion &amp; Preparation), <code>ShinyOlink</code> automatically updates the dropdown menus in all the analysis modules (like T-Test or ANOVA) with the new column names from your merged dataset.</li> <li>When you run a T-Test (from Chapter 2: Statistical Analysis &amp; Visualization Modules), the app automatically makes those results available, and the Volcano Plot module then knows exactly which data to use to draw the plot.</li> </ul> <p>This makes the <code>ShinyOlink</code> app seamless and intuitive, just like that responsive spreadsheet.</p>"},{"location":"chapter6/#key-concepts-the-brains-connections","title":"Key Concepts: The Brain's Connections","text":"<p>Shiny applications are built on this idea of \"reactivity.\" This means that when certain pieces of information (inputs) change, other pieces of the application that depend on them (outputs or calculations) automatically update. Two fundamental tools make this happen:</p>"},{"location":"chapter6/#1-reactiveval-the-dynamic-whiteboard","title":"1. <code>reactiveVal</code>: The Dynamic Whiteboard","text":"<p><code>reactiveVal</code> is a special type of variable in Shiny that can store data that changes over time. Think of it like a shared whiteboard that different parts of the app can read from and write to. When someone writes new information on this whiteboard, everyone else who is \"watching\" it automatically knows there's an update.</p> <ul> <li>Purpose: To hold dynamic data that needs to be shared and updated across different parts of your Shiny app.</li> <li>Analogy: A \"sticky note\" or a shared \"whiteboard\" where the app stores important, changing information, like your <code>merged_data</code> or the <code>ttest_results</code>.</li> <li>Example in <code>ShinyOlink</code>:<ul> <li><code>merged_data</code>: This <code>reactiveVal</code> holds your combined Olink dataset after you upload and merge your files.</li> <li><code>ttest_results</code>: This <code>reactiveVal</code> stores the statistical results generated by the T-Test module.</li> </ul> </li> </ul>"},{"location":"chapter6/#2-observeevent-the-smart-listener","title":"2. <code>observeEvent</code>: The Smart Listener","text":"<p><code>observeEvent</code> is a special function that listens for specific actions or changes and then runs a block of code in response. It's like having a dedicated sensor that only triggers when something particular happens.</p> <ul> <li>Purpose: To run code only when a specific \"event\" occurs (e.g., a button is clicked, or a <code>reactiveVal</code> changes its value).</li> <li>Analogy: A \"sensor\" or \"alarm\" that goes off only when you press a specific button, or when someone updates a specific whiteboard (<code>reactiveVal</code>).</li> <li>Example in <code>ShinyOlink</code>:<ul> <li><code>observeEvent(input$merge_data, {...})</code>: This listens for you to click the \"Merge Data\" button. When you click it, the code inside the curly braces runs to merge your data.</li> <li><code>observeEvent(input$run_ttest, {...})</code>: This listens for you to click the \"Run T-Test\" button. When clicked, it runs the code to perform the T-test calculation.</li> </ul> </li> </ul>"},{"location":"chapter6/#how-they-work-together","title":"How They Work Together","text":"<p><code>observeEvent</code> often works with <code>reactiveVal</code>. An <code>observeEvent</code> might trigger when a button is clicked, and then the code it runs might update a <code>reactiveVal</code>. Other parts of the app, in turn, can read from that <code>reactiveVal</code> to perform their own actions, automatically reacting to the change. This creates a chain of reactions throughout the app.</p>"},{"location":"chapter6/#how-shinyolink-uses-reactive-programming","title":"How <code>ShinyOlink</code> Uses Reactive Programming","text":"<p>Let's trace our familiar examples to see reactive programming in action.</p>"},{"location":"chapter6/#scenario-1-data-upload-variable-updates","title":"Scenario 1: Data Upload -&gt; Variable Updates","text":"<ol> <li>You upload your NPX and Variables files and click the \"Merge Data\" button.</li> <li>The <code>ShinyOlink</code> app uses an <code>observeEvent</code> to detect this button click.</li> <li>Inside that <code>observeEvent</code>, the app reads your files, performs the merging, and then updates the <code>merged_data</code> <code>reactiveVal</code> with your new, combined dataset.</li> <li>Crucially, other parts of the app that rely on <code>merged_data</code> automatically \"notice\" this change. For instance, the dropdown menus in the T-Test, ANOVA, and PCA modules, which need to show the column names from your data, automatically update to display the new columns from your freshly merged <code>merged_data</code>. You don't have to click a \"refresh variables\" button!</li> </ol>"},{"location":"chapter6/#scenario-2-t-test-results-volcano-plot-readiness","title":"Scenario 2: T-Test Results -&gt; Volcano Plot Readiness","text":"<ol> <li>You click the \"Run T-Test\" button in the T-Test module.</li> <li>An <code>observeEvent</code> detects this click.</li> <li>The T-Test calculations are performed using your <code>merged_data</code>. The results (p-values, fold changes) are then stored in the <code>ttest_results</code> <code>reactiveVal</code>.</li> <li>Now, when you go to the Volcano Plot module and click \"Generate Volcano Plot,\" it immediately reads from the <code>ttest_results</code> <code>reactiveVal</code> to get the latest statistical output. The Volcano Plot doesn't need to re-run the T-test; it just uses the ready-made results, ensuring efficiency and consistency.</li> </ol> <p>This continuous flow of information, where changes in one place automatically trigger updates in others, is the essence of reactive programming in Shiny.</p>"},{"location":"chapter6/#under-the-hood-the-automatic-chain-reaction","title":"Under the Hood: The Automatic Chain Reaction","text":"<p>Let's look at a simplified walkthrough of how these reactive elements connect behind the scenes.</p>"},{"location":"chapter6/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>You Interact: You click the \"Merge Data\" button in the <code>ShinyOlink</code> User Interface (UI).</li> <li>Event Detected: The <code>ShinyOlink</code> Server's <code>observeEvent</code> listening for <code>input$merge_data</code> detects your click and begins its associated code.</li> <li>Data Updated: This code performs the file merging and then updates the <code>merged_data</code> <code>reactiveVal</code> with the new dataset.</li> <li>Dependencies React: Any part of the app (including <code>updateSelectInput</code> calls that refresh dropdown choices) that uses <code>merged_data()</code> automatically reacts to this change and updates itself.</li> <li>You Interact Again: You then select a grouping variable in the T-Test module and click \"Run T-Test.\"</li> <li>Event Detected: The <code>observeEvent</code> listening for <code>input$run_ttest</code> in the T-Test module detects this click.</li> <li>Calculations Performed: This code uses the current <code>merged_data()</code> (which it automatically gets from the <code>reactiveVal</code>) to perform the T-test calculation.</li> <li>Results Updated: The T-test results are then saved into the <code>ttest_results</code> <code>reactiveVal</code>.</li> <li>Dependencies React: The Volcano Plot module, which is set up to use <code>ttest_results()</code>, now knows that fresh T-test results are available and is ready to generate the plot.</li> </ol> <p>Here's a simple diagram to visualize this reactive flow:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Shiny App (UI)\n    participant Shiny App (Server)\n    participant merged_data (reactiveVal)\n    participant ttest_results (reactiveVal)\n\n    User-&gt;&gt;Shiny App (UI): 1. Clicks \"Merge Data\"\n    Shiny App (UI)-&gt;&gt;Shiny App (Server): 2. Sends input$merge_data signal\n    Shiny App (Server)-&gt;&gt;Shiny App (Server): 3. observeEvent(input$merge_data) triggers\n    Shiny App (Server)-&gt;&gt;merged_data: 4. merged_data(new_dataset)\n    Note over Shiny App (Server): 5. Other parts (e.g., dropdowns) automatically update\n    Shiny App (Server)--&gt;&gt;Shiny App (UI): 6. Updates variable dropdowns in UI\n\n    User-&gt;&gt;Shiny App (UI): 7. Clicks \"Run T-Test\"\n    Shiny App (UI)-&gt;&gt;Shiny App (Server): 8. Sends input$run_ttest signal\n    Shiny App (Server)-&gt;&gt;Shiny App (Server): 9. observeEvent(input$run_ttest) triggers (uses merged_data())\n    Shiny App (Server)-&gt;&gt;ttest_results: 10. ttest_results(new_results)\n    Note over Shiny App (Server): 11. Volcano Plot module knows new results are ready\n    Shiny App (Server)--&gt;&gt;Shiny App (UI): 12. Displays T-Test table, enables plot generation</code></pre>"},{"location":"chapter6/#diving-into-the-code-reactive-components","title":"Diving into the Code: Reactive Components","text":"<p>Let's look at simplified code snippets showing how <code>reactiveVal</code> and <code>observeEvent</code> are used.</p> <p>First, in <code>app/app.R</code>, you declare your main <code>reactiveVal</code> containers. Think of this as setting up your whiteboards for the app:</p> <p><pre><code># app/app.R (Simplified)\n# ... (other code) ...\n\nserver &lt;- function(input, output, session) {\n  # These are our dynamic whiteboards (reactiveVal containers)\n  merged_data &lt;- reactiveVal(NULL) # Starts empty\n  ttest_results &lt;- reactiveVal(NULL) # Starts empty\n\n  # ... (more server logic) ...\n}\n</code></pre> Explanation: <code>merged_data &lt;- reactiveVal(NULL)</code> creates a special container named <code>merged_data</code> that can hold data, and it starts with nothing inside (<code>NULL</code>). The same applies to <code>ttest_results</code>. These <code>reactiveVal</code>s are passed around to different modules so they can share information.</p> <p>Next, in <code>app/server/server_data_input.R</code>, you see how <code>merged_data</code> is updated after file merging:</p> <p><pre><code># app/server/server_data_input.R (Simplified)\ndata_input_server &lt;- function(input, output, session, merged_data, var_key_merged) {\n  observeEvent(input$merge_data, { # This is our \"smart listener\" for the button click\n    req(input$npx_file, input$var_file) # Makes sure files are uploaded\n\n    # ... (code to read and merge files into a 'merged' variable) ...\n\n    merged_data(merged) # WRITE to the 'merged_data' reactiveVal whiteboard!\n\n    # These lines automatically update dropdowns because merged_data changed\n    updateSelectInput(session, \"pca_var\", choices = colnames(merged))\n    updateSelectInput(session, \"ttest_var\", choices = colnames(merged))\n    # ... (more updates) ...\n  })\n}\n</code></pre> Explanation: The <code>observeEvent(input$merge_data, {...})</code> waits for the \"Merge Data\" button click. Once clicked, the code inside runs. <code>merged_data(merged)</code> is the key line: it writes the newly combined <code>merged</code> dataset onto the <code>merged_data</code> whiteboard. Because this whiteboard was updated, other inputs like <code>updateSelectInput</code> (which dynamically updates the options in dropdown menus) automatically react and display the new column names found in <code>merged_data</code>.</p> <p>Now, in <code>app/server/server_ttest.R</code>, you see how <code>ttest_results</code> is updated:</p> <p><pre><code># app/server/server_ttest.R (Simplified)\nttest_server &lt;- function(input, output, session, merged_data, ttest_results) {\n  observeEvent(input$run_ttest, { # Listener for the T-Test button\n    req(merged_data(), input$ttest_var) # Requires merged_data to be available\n\n    data_for_test &lt;- merged_data() # READ from the 'merged_data' reactiveVal whiteboard!\n\n    # ... (code to prepare data for test) ...\n\n    results &lt;- olink_ttest(data_for_test, variable = input$ttest_var)\n\n    ttest_results(results) # WRITE to the 'ttest_results' reactiveVal whiteboard!\n\n    # ... (code to display results table) ...\n  })\n}\n</code></pre> Explanation: <code>observeEvent(input$run_ttest, {...})</code> listens for the \"Run T-Test\" button. <code>data_for_test &lt;- merged_data()</code> reads the latest combined dataset from the <code>merged_data</code> whiteboard. After the <code>olink_ttest</code> function calculates the results, <code>ttest_results(results)</code> writes these new results onto the <code>ttest_results</code> whiteboard, making them available to other modules, like the Volcano Plot.</p> <p>Finally, in <code>app/server/server_volcano_plot.R</code>, the Volcano Plot module simply reads the T-Test results:</p> <p><pre><code># app/server/server_volcano_plot.R (Simplified)\nvolcano_plot_server &lt;- function(input, output, session, merged_data, ttest_results, anova_results) {\n  observeEvent(input$run_volcano, { # Listener for \"Generate Volcano Plot\"\n    if (input$volcano_plot_type == \"ttest\") {\n      req(ttest_results()) # Makes sure T-Test results exist on the whiteboard\n      results &lt;- ttest_results() # READ from the 'ttest_results' reactiveVal whiteboard!\n    } else {\n      # ... (code for ANOVA results) ...\n    }\n\n    plot &lt;- olink_volcano_plot(results) # Create the plot using the results\n    # ... (code to display plot) ...\n  })\n}\n</code></pre> Explanation: When you click \"Generate Volcano Plot,\" the <code>observeEvent</code> triggers. If \"T-test\" is selected, <code>results &lt;- ttest_results()</code> reads the latest T-test results directly from the <code>ttest_results</code> whiteboard. The <code>olink_volcano_plot</code> then immediately uses these results to draw the graph. This shows how <code>reactiveVal</code>s effectively pass data around the application, allowing different parts to react and update automatically.</p>"},{"location":"chapter6/#conclusion","title":"Conclusion","text":"<p>In this chapter, you've grasped the core concept of Reactive Programming in <code>ShinyOlink</code>:</p> <ul> <li>It's the mechanism that makes your app dynamic and responsive, automatically updating outputs when inputs change, much like a spreadsheet.</li> <li>You learned about two key components: <code>reactiveVal</code> (the \"dynamic whiteboard\" for storing changing data) and <code>observeEvent</code> (the \"smart listener\" that triggers code when specific actions occur).</li> <li>You saw how these work together to allow data to flow seamlessly through the application, enabling features like automatic variable updates after data ingestion and plot generation from fresh analysis results.</li> </ul> <p>Understanding reactivity is fundamental to truly grasping how <code>ShinyOlink</code> (and any Shiny application) functions. Now that you know how the app responds to your actions, let's look at how such an application is prepared for others to use, even outside of your own computer.</p> <p>Ready to see how <code>ShinyOlink</code> can be shared with the world? Let's move on to Chapter 7: Automated Deployment Pipelines!</p>"},{"location":"chapter7/","title":"Chapter 7: Automated Deployment Pipelines","text":"<p>Welcome back, <code>ShinyOlink</code> enthusiast! In Chapter 6: Reactive Programming, you learned how <code>ShinyOlink</code> uses \"reactivity\" to make different parts of the app automatically update and respond to your actions, creating a smooth and interactive experience. You now understand how the app works on your computer.</p> <p>But what happens when a developer makes a new improvement or fixes a bug in <code>ShinyOlink</code>? How do those changes get from their computer to the public website where everyone can use the updated version? Doing this manually for every change would be slow, prone to errors, and take a lot of effort!</p>"},{"location":"chapter7/#what-are-automated-deployment-pipelines","title":"What are \"Automated Deployment Pipelines\"?","text":"<p>Imagine you run a factory that produces a popular product. Every time you have a new and improved version, you wouldn't want to manually gather all the parts, assemble them, test them, and then personally deliver each one to your customers. That would be chaotic! Instead, you'd want an automated factory line that takes your new design, builds it, checks its quality, and ships it out efficiently and reliably.</p> <p>The main problem \"Automated Deployment Pipelines\" solve is this: How do we automatically take the latest changes to the <code>ShinyOlink</code> code, build a ready-to-use version of the application, test it, and then make it available for everyone to use online, all without tedious manual steps?</p> <p>In <code>ShinyOlink</code>, this \"automated factory line\" is called an Automated Deployment Pipeline. When a developer writes new code and saves it to the project's central storage (GitHub), this pipeline automatically kicks into action. It ensures that the <code>ShinyOlink</code> application is built into a self-contained package (a \"Docker image\") and then released to a web server for you to use. This makes updates faster, more reliable, and less prone to human error.</p> <p>Let's use a common scenario: A developer adds a new statistical analysis feature to <code>ShinyOlink</code>. Their goal is to make this new feature available on the public <code>ShinyOlink</code> website for users as quickly and smoothly as possible. The automated deployment pipeline handles everything after the developer finishes writing the code!</p>"},{"location":"chapter7/#key-concepts-the-automated-factory-tools","title":"Key Concepts: The Automated Factory Tools","text":"<p>Automated deployment pipelines rely on a few key tools and ideas to work their magic:</p>"},{"location":"chapter7/#1-github-actions-the-automated-worker","title":"1. GitHub Actions: The Automated Worker","text":"Concept Description Analogy GitHub Actions A service provided by GitHub (where <code>ShinyOlink</code> code is stored) that allows you to automate tasks directly within your code repository. It's like having a robot worker that performs tasks for you. The robot on the assembly line. Workflows These are instructions (written in YAML files) that tell GitHub Actions what to do, and when to do it. They define the \"assembly line\" steps. The detailed instructions for the robot."},{"location":"chapter7/#2-docker-the-standardized-shipping-container","title":"2. Docker: The Standardized Shipping Container","text":"Concept Description Analogy Docker A technology that allows you to package an application and all its necessary parts (code, libraries, settings) into a single, standardized unit called an image. A standardized shipping container. Docker Image A self-contained, lightweight, and executable package that has everything needed to run a piece of software. It's like a complete, pre-assembled product. The product, neatly packed in its container, ready to be shipped anywhere. Dockerfile A simple text file that contains instructions on how to build a Docker image (e.g., \"start with R, then add these packages, then add the app code\"). The instruction manual for building the product inside the container."},{"location":"chapter7/#3-docker-hub-the-warehouse","title":"3. Docker Hub: The Warehouse","text":"Concept Description Analogy Docker Hub A cloud-based repository (like a big online storage) for Docker images. The warehouse where all the ready-to-ship products (Docker images) are stored."},{"location":"chapter7/#how-to-use-automated-deployment-pipelines-as-a-developer","title":"How to Use Automated Deployment Pipelines (as a Developer)","text":"<p>For a developer working on <code>ShinyOlink</code>, using these pipelines is remarkably simple:</p> <ol> <li>Write Code: The developer writes new R code for a feature or bug fix.</li> <li>Save &amp; Push: They save their changes and \"push\" them to the <code>ShinyOlink</code> project on GitHub.</li> </ol> <p>That's it! Once the code is pushed to GitHub, the automated pipeline takes over. The developer doesn't need to manually run any commands to build the Docker image or deploy the app to the web. The GitHub Actions robots handle everything, making sure the latest <code>ShinyOlink</code> is always available online.</p>"},{"location":"chapter7/#under-the-hood-the-automatic-journey","title":"Under the Hood: The Automatic Journey","text":"<p>Let's trace the journey of <code>ShinyOlink</code> from a developer's computer to the public website, step-by-step.</p>"},{"location":"chapter7/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>Developer Pushes Code: A <code>ShinyOlink</code> developer finishes their work and pushes new code changes to the <code>main</code> branch of the <code>ShinyOlink</code> project on GitHub.</li> <li> <p>GitHub Actions Triggers: GitHub detects these new changes. Since the project has pre-defined \"workflows\" (our instructions for the robots), GitHub Actions automatically starts two separate pipelines:</p> <ul> <li>Pipeline 1: Docker Image Build &amp; Push: This pipeline focuses on creating a new, updated Docker image for <code>ShinyOlink</code>. It tells Docker how to package the app and then sends it to Docker Hub.</li> <li>Pipeline 2: ShinyApps.io Deployment: This pipeline takes the latest app code and deploys it directly to the ShinyApps.io platform, making it accessible as a public website.</li> </ul> </li> <li> <p>Inside Pipeline 1 (Docker Build):</p> <ul> <li>GitHub Actions checks out (gets a copy of) the latest <code>ShinyOlink</code> code.</li> <li>It reads the <code>Dockerfile</code> (the building instructions) and uses Docker to create a new, self-contained <code>ShinyOlink</code> image. This image includes R, all necessary R packages (like <code>OlinkAnalyze</code> from Chapter 3: OlinkAnalyze Package Integration), and the <code>ShinyOlink</code> application code itself.</li> <li>Once the image is built, GitHub Actions logs into Docker Hub and \"pushes\" the newly built image to the <code>ShinyOlink</code> repository there, making it available in the \"warehouse.\"</li> </ul> </li> <li> <p>Inside Pipeline 2 (ShinyApps.io Deploy):</p> <ul> <li>GitHub Actions again checks out the latest <code>ShinyOlink</code> code.</li> <li>It sets up R and installs all the specific R packages that <code>ShinyOlink</code> needs to run (like <code>shiny</code>, <code>rsconnect</code>, and many others).</li> <li>It then uses special credentials (secrets, kept safe on GitHub) to log into the <code>ShinyApps.io</code> deployment service.</li> <li>Finally, it instructs <code>ShinyApps.io</code> to deploy the <code>ShinyOlink</code> application using the latest code. This updates the public website with the new version.</li> </ul> </li> </ol> <p>This entire process, from code push to live application, happens automatically, saving a lot of time and effort!</p> <p>Here's a simple diagram to visualize this automated flow:</p> <pre><code>sequenceDiagram\n    participant Developer\n    participant GitHub\n    participant GitHub Actions\n    participant Docker Hub\n    participant ShinyApps.io (Website)\n\n    Developer-&gt;&gt;GitHub: 1. Pushes new code\n    GitHub-&gt;&gt;GitHub Actions: 2. Triggers Docker Build Pipeline\n    GitHub Actions-&gt;&gt;GitHub Actions: 3. Checks out code\n    Note over GitHub Actions: Builds Docker Image (using Dockerfile)\n    GitHub Actions-&gt;&gt;Docker Hub: 4. Pushes new Docker Image\n    GitHub-&gt;&gt;GitHub Actions: 5. Also Triggers ShinyApps.io Deploy Pipeline\n    GitHub Actions-&gt;&gt;GitHub Actions: 6. Checks out code &amp; Installs R pkgs\n    Note over GitHub Actions: Connects to ShinyApps.io\n    GitHub Actions-&gt;&gt;ShinyApps.io (Website): 7. Deploys updated app\n    ShinyApps.io (Website)-&gt;&gt;User: User accesses latest ShinyOlink</code></pre>"},{"location":"chapter7/#diving-into-the-code-the-workflow-files","title":"Diving into the Code: The Workflow Files","text":"<p>The instructions for these automated pipelines are defined in special YAML files located in the <code>.github/workflows/</code> folder of the <code>ShinyOlink</code> project. Let's look at simplified parts of these files.</p>"},{"location":"chapter7/#1-docker-image-build-workflow-githubworkflowsdocker-buildyml","title":"1. Docker Image Build Workflow (<code>.github/workflows/docker-build.yml</code>)","text":"<p>This file tells GitHub Actions how to build and push the Docker image to Docker Hub:</p> <p><pre><code># .github/workflows/docker-build.yml (Simplified)\nname: docker-image\n\non:\n  push: # This pipeline runs every time new code is pushed\n\njobs:\n  docker:\n    runs-on: ubuntu-latest # Run this job on a standard Ubuntu machine\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4 # Get the latest code from GitHub\n      - name: Login to Docker Hub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }} # Use secret credentials for login\n      - name: Build and push\n        uses: docker/build-push-action@v6 # This action builds and pushes the Docker image\n        with:\n          context: . # Look for the Dockerfile in the current folder\n          push: true # Yes, push the image to Docker Hub\n          tags: |\n            jd21/shinyolink:latest # Tag the image as 'latest'\n            # ... other tags like date-based versions ...\n</code></pre> Explanation: *   <code>on: push:</code>: This is the trigger. It means this \"job\" will run automatically every time a developer pushes new code to any branch. *   <code>uses: actions/checkout@v4</code>: This step is like telling the robot to \"get a copy of all the project files.\" *   <code>Login to Docker Hub</code>: This step uses secure \"secrets\" (which are configured in GitHub and not visible in the code) to log into Docker Hub. *   <code>Build and push</code>: This is the core step. It uses a special pre-made \"action\" (<code>docker/build-push-action</code>) to build the Docker image using the <code>Dockerfile</code> in the project and then push it to <code>jd21/shinyolink</code> on Docker Hub, labeling it as <code>latest</code>.</p>"},{"location":"chapter7/#the-dockerfile-dockerfile","title":"The Dockerfile (<code>Dockerfile</code>)","text":"<p>This file contains the actual instructions for building the <code>ShinyOlink</code> Docker image:</p> <p><pre><code># Dockerfile (Simplified)\nFROM jd21/shinyolink:v1.2 # Start building from a specific base R image\n\nUSER root\n\n# Install system dependencies needed by R packages\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    libcurl4-openssl-dev \\\n    # ... more system libraries ...\n\n# Install R packages needed for ShinyOlink\nRUN R -e \"\\\n    install.packages('BiocManager', repos='https://cran.rstudio.com/'); \\\n    BiocManager::install(version = '3.20'); \\\n    # ... many more R package installations ...\n    \"\n\n# Create a folder inside the Docker image for the app\nRUN mkdir -p /srv/shiny-server/app_cache &amp;&amp; \\\n    chown -R shiny:shiny /srv/shiny-server/app_cache\n\n# Copy the ShinyOlink application files into the image\nCOPY app/ /srv/shiny-server/\n\n# Tell the Docker image that the app will listen on port 3838\nEXPOSE 3838\n\n# When the Docker image runs, start the Shiny server\nCMD [\"/usr/bin/shiny-server\"]\n</code></pre> Explanation: *   <code>FROM jd21/shinyolink:v1.2</code>: This tells Docker to start building the image from an existing base image that already has R installed. It's like starting with a pre-built foundation. *   <code>RUN ...</code>: These lines execute commands inside the Docker image during its creation. They install necessary system libraries (like tools for handling secure internet connections) and then install all the R packages <code>ShinyOlink</code> depends on (like <code>shiny</code>, <code>OlinkAnalyze</code>, <code>clusterProfiler</code>, etc.). *   <code>COPY app/ /srv/shiny-server/</code>: This copies all the <code>ShinyOlink</code> application's R files (UI, server, modules, etc.) into the correct location within the Docker image. *   <code>EXPOSE 3838</code>: This tells the outside world that this Docker container will serve a web application on port 3838. *   <code>CMD [\"/usr/bin/shiny-server\"]</code>: This is the command that runs automatically when someone starts a container from this Docker image. It starts the Shiny server, which then runs <code>ShinyOlink</code>.</p>"},{"location":"chapter7/#the-shinyappsio-deployment-workflow-githubworkflowsshinyapps_deployyml","title":"The ShinyApps.io Deployment Workflow (<code>.github/workflows/shinyapps_deploy.yml</code>)","text":"<p>This file specifies how <code>ShinyOlink</code> is deployed to the public ShinyApps.io platform:</p> <p><pre><code># .github/workflows/shinyapps_deploy.yml (Simplified)\nname: shinyapps-deployment\non:\n    push:\n        branches:\n            - main # This pipeline only runs when code is pushed to the 'main' branch\n\njobs:\n  test:\n      name: shinyio deployment\n      runs-on: ubuntu-latest # Run on a standard Ubuntu machine\n      steps:\n      - uses: r-lib/actions/setup-r@v2-branch # Set up R environment\n      - name: Install rsconnect\n        run: |\n          R -e 'install.packages(\"rsconnect\")' # Install the package to deploy to ShinyApps.io\n          # ... install other ShinyOlink runtime packages ...\n      - uses: actions/checkout@v4 # Get the latest code\n      - name: Connect shiny server\n        run: Rscript -e \"rsconnect::setAccountInfo(name=${{ secrets.SHINYIO_NAME }}, token=${{ secrets.SHINYIO_TOKEN }}, secret=${{ secrets.SHINYIO_SECRET }})\" # Use secrets to log in\n      - name: deploy app \n        run: Rscript -e \"rsconnect::deployApp(appName = 'ShinyOlink')\" # Deploy the app!\n</code></pre> Explanation: *   <code>on: push: branches: - main</code>: This pipeline specifically triggers only when code is pushed to the <code>main</code> branch, ensuring only tested and approved changes go to the live website. *   <code>setup-r@v2-branch</code>: This step sets up a working R environment for the deployment. *   <code>Install rsconnect</code>: This installs <code>rsconnect</code>, which is the R package specifically designed for deploying Shiny applications to ShinyApps.io. It also installs other packages <code>ShinyOlink</code> needs to run. *   <code>Connect shiny server</code>: This uses secret credentials to connect to the ShinyApps.io account. *   <code>deploy app</code>: This is the crucial command that tells <code>rsconnect</code> to take all the <code>ShinyOlink</code> app files and deploy them to the \"ShinyOlink\" application on the ShinyApps.io server. If an app with that name already exists, it will update it; otherwise, it will create a new one.</p> <p>These workflow files are <code>ShinyOlink</code>'s \"robot instructions.\" They ensure that every time a developer makes a change to the <code>main</code> branch, a new, updated version of <code>ShinyOlink</code> is built and deployed automatically, making it available for everyone to use with confidence.</p>"},{"location":"chapter7/#conclusion","title":"Conclusion","text":"<p>In this chapter, you've gained an understanding of Automated Deployment Pipelines in the context of <code>ShinyOlink</code>:</p> <ul> <li>You learned that pipelines are like an automated factory line that takes code changes and automatically builds, tests (conceptually, though testing steps are simplified here), and deploys the application.</li> <li>You explored key tools: GitHub Actions (the robot worker), Docker (the standardized shipping container), and Docker Hub (the warehouse).</li> <li>You saw how <code>ShinyOlink</code> uses two main pipelines: one for building and pushing a Docker image, and another for deploying the application to <code>ShinyApps.io</code>.</li> <li>You got a simplified look at the YAML workflow files and the <code>Dockerfile</code> that orchestrate this automatic process.</li> </ul> <p>This automated approach ensures that <code>ShinyOlink</code> updates are delivered quickly, reliably, and consistently, letting developers focus on new features and you, the user, always get the latest version.</p> <p>Now that you understand how <code>ShinyOlink</code> is built and delivered, let's explore how the project is managed and how you can contribute to its future.</p> <p>Ready to be part of the <code>ShinyOlink</code> community? Let's move on to Chapter 8: Project Governance &amp; Contributions!</p>"},{"location":"chapter8/","title":"Chapter 8: Project Governance &amp; Contributions","text":"<p>Welcome back, <code>ShinyOlink</code> enthusiast! In Chapter 7: Automated Deployment Pipelines, we explored how <code>ShinyOlink</code> is automatically built, tested, and deployed, ensuring you always get the latest version. This process is all about the code and its journey from a developer's computer to your web browser.</p> <p>But a successful software project isn't just about code; it's also about people \u2013 the developers who build it, the users who use it, and anyone who wants to help make it better. How do we ensure everyone can interact, propose ideas, fix issues, and contribute in a smooth, respectful, and organized way?</p>"},{"location":"chapter8/#what-is-project-governance-contributions","title":"What is \"Project Governance &amp; Contributions\"?","text":"<p>Imagine <code>ShinyOlink</code> is a shared community garden. Many people want to plant things, help take care of it, or suggest new flowers. If there are no rules or guidelines, it could become messy quickly, with people planting anywhere or arguing about what to grow.</p> <p>The main problem \"Project Governance &amp; Contributions\" solves is this: How do we create a clear \"rulebook\" and a \"customer service\" department for <code>ShinyOlink</code>? This ensures the project stays healthy, high-quality, and is a welcoming place for everyone involved, whether they're reporting a tiny bug or proposing a huge new feature.</p> <p>Let's use a common scenario: You're using <code>ShinyOlink</code> and discover a small bug, or you have a brilliant idea for a new analysis feature. How do you tell the project team? Or, perhaps you're a developer yourself and want to contribute a code fix. How do you do it in a way that helps the project, rather than causing confusion? This chapter will guide you through the \"rules of the road\" for <code>ShinyOlink</code>.</p>"},{"location":"chapter8/#key-concepts-the-projects-rulebook","title":"Key Concepts: The Project's Rulebook","text":"<p><code>ShinyOlink</code> (like many open-source projects) uses specific documents and processes to manage its community and contributions. Think of these as different sections of the project's \"rulebook\":</p> Document Type What it Is Analogy Purpose Issue Templates Pre-formatted forms for reporting bugs or suggesting new features. \"Help Desk Tickets\" with specific questions. Ensures clear, consistent information for problems/ideas. Contributing Guide A document explaining how to propose code changes or other contributions. \"How-to Guide\" for helping the project. Guides contributors on the best way to submit their work. Code of Conduct Rules for respectful and positive behavior in the community. \"Community Agreement\" on good manners. Fosters a welcoming and safe environment for everyone. Security Policy Guidelines for how to report security vulnerabilities privately. \"Secret Hotline\" for urgent safety issues. Ensures sensitive issues are handled responsibly and quickly. <p>These documents help manage the \"health\" of the <code>ShinyOlink</code> project, much like the factory line (from Chapter 7) manages the code's health.</p>"},{"location":"chapter8/#how-to-interact-with-shinyolink-your-use-case","title":"How to Interact with <code>ShinyOlink</code> (Your Use Case)","text":"<p>Let's walk through how you, as a user or potential contributor, would interact with these guidelines. The primary place for these interactions is typically the project's GitHub repository, which is where <code>ShinyOlink</code>'s code lives.</p>"},{"location":"chapter8/#scenario-1-you-found-a-bug-or-have-a-new-idea","title":"Scenario 1: You Found a Bug or Have a New Idea!","text":"<p>If you find something broken or think of a cool new feature, you'd typically go to the \"Issues\" tab on the <code>ShinyOlink</code> GitHub page.</p> <ol> <li> <p>Report a Bug:</p> <ul> <li>You'd click \"New issue\" and then choose a \"Bug report\" template.</li> <li>This template asks you specific questions like \"What is the bug?\", \"How can we reproduce it?\", and \"What did you expect to happen?\". This helps the developers understand and fix the problem quickly.</li> </ul> <p>Here's a simplified look at the kind of information the <code>Bug report</code> template (<code>.github/ISSUE_TEMPLATE/bug_report.md</code>) asks for:</p> <p><pre><code># Bug report\n**Describe the bug**\nA clear and concise description of what the bug is.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Go to '...'\n2. Click on '....'\n3. Scroll down to '....'\n4. See error\n\n**Expected behavior**\nA clear and concise description of what you expected to happen.\n\n**Desktop (please complete the following information):**\n - OS: [e.g. iOS]\n - Browser [e.g. chrome, safari]\n</code></pre> This ensures you provide all the necessary details, similar to a good doctor asking for symptoms.</p> </li> <li> <p>Request a Feature:</p> <ul> <li>Similarly, you'd choose a \"Feature request\" template.</li> <li>This template guides you to describe the problem your idea solves, your proposed solution, and any alternatives you've considered.</li> </ul> <p>Here's a simplified view of the <code>Feature request</code> template (<code>.github/ISSUE_TEMPLATE/feature_request.md</code>):</p> <p><pre><code># Feature request\n**Is your feature request related to a problem? Please describe.**\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n\n**Describe the solution you'd like**\nA clear and concise description of what you want to happen.\n\n**Describe alternatives you've considered**\nA clear and concise description of any alternative solutions or features you've considered.\n</code></pre> This helps the <code>ShinyOlink</code> team understand the value of your idea and discuss it effectively.</p> </li> </ol>"},{"location":"chapter8/#scenario-2-you-want-to-contribute-code","title":"Scenario 2: You Want to Contribute Code!","text":"<p>If you're interested in directly helping improve <code>ShinyOlink</code> by writing code (e.g., fixing a bug, adding a small feature), you'll want to consult the Contributing Guide.</p> <ol> <li> <p>Read the Contributing Guide:</p> <ul> <li>This document (<code>CONTRIBUTION/CONTRIBUTING.md</code>) acts as your roadmap. It explains the best way to get your changes into the project.</li> <li>It covers topics like how to \"fork\" the project (make your own copy), make your changes, and then submit a \"Pull Request\" (PR) \u2013 which is how you propose your changes to the original project.</li> </ul> <p>The Contributing Guide starts with a clear welcome:</p> <p><pre><code># Contributing Guidelines\n\n*Pull requests, bug reports, and all other forms of contribution are welcomed and highly encouraged!* :octocat:\n\n### Contents\n\n- [Code of Conduct](#book-code-of-conduct)\n- [Asking Questions](#bulb-asking-questions)\n- [Opening an Issue](#inbox_tray-opening-an-issue)\n# ... much more ...\n</code></pre> It's your first stop for understanding how to help!</p> </li> <li> <p>Submit a Pull Request (PR):</p> <ul> <li>After you make your changes, you'd create a Pull Request on GitHub. This is like saying, \"Hey, I've got some changes here. Can you review them and add them to the main project?\"</li> <li>The Contributing Guide often gives tips on writing good PR descriptions and commit messages.</li> </ul> </li> </ol>"},{"location":"chapter8/#scenario-3-being-a-good-community-member","title":"Scenario 3: Being a Good Community Member","text":"<p>All interactions within the <code>ShinyOlink</code> community (whether on GitHub, in discussions, or in code reviews) are expected to follow the Code of Conduct.</p> <ol> <li> <p>Understand the Code of Conduct:</p> <ul> <li>This document (<code>CODE_OF_CONDUCT.md</code>) clearly outlines expected behaviors (e.g., being respectful, providing constructive feedback) and unacceptable behaviors (e.g., harassment, insulting comments).</li> <li>It also describes how any violations will be handled to ensure a safe and positive environment for everyone.</li> </ul> <p>The <code>ShinyOlink</code> Code of Conduct emphasizes a positive environment:</p> <p><pre><code># Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n# ... more standards and enforcement details ...\n</code></pre> This document is crucial for maintaining a healthy community.</p> </li> </ol>"},{"location":"chapter8/#scenario-4-you-found-a-security-vulnerability-important","title":"Scenario 4: You Found a Security Vulnerability (Important!)","text":"<p>If you ever discover a potential security issue in <code>ShinyOlink</code> (e.g., a way to access private data or break the application in a harmful way), it's critical to report it privately, not publicly.</p> <ol> <li> <p>Consult the Security Policy:</p> <ul> <li>The <code>SECURITY.md</code> file tells you exactly how to report such issues. For <code>ShinyOlink</code>, it explicitly states not to file a public issue, but to send an email to a specific address (<code>oss@jessesquires.com</code>).</li> <li>This allows the developers to fix the vulnerability before malicious actors can exploit it.</li> </ul> <p>The <code>ShinyOlink</code> Security Policy is very clear:</p> <p><pre><code># Security Policy\n\nIf you discover a security issue, please bring it to our attention right away!\n\n## Reporting a Vulnerability\n\nPlease **DO NOT** file a public issue to report a security vulberability, instead send your report privately to **oss@jessesquires.com**. This will help ensure that any vulnerabilities that are found can be [disclosed responsibly](https://en.wikipedia.org/wiki/Responsible_disclosure) to any affected parties.\n</code></pre> Following this policy ensures that <code>ShinyOlink</code> remains safe for everyone.</p> </li> </ol>"},{"location":"chapter8/#under-the-hood-where-the-rulebook-lives","title":"Under the Hood: Where the Rulebook Lives","text":"<p>These important \"rulebook\" documents are not just ideas; they are actual files stored within the <code>ShinyOlink</code> project on GitHub. They are typically located in special, easy-to-find places.</p>"},{"location":"chapter8/#step-by-step-location-guide","title":"Step-by-Step Location Guide","text":"<p>When you visit the <code>ShinyOlink</code> project on GitHub, you'll find these governance documents in specific folders:</p> <ol> <li>Issue Templates: These live in the <code>.github/ISSUE_TEMPLATE/</code> folder. GitHub automatically uses these files when someone clicks \"New Issue.\"</li> <li>Code of Conduct: The <code>CODE_OF_CONDUCT.md</code> file is usually found right at the top level of the project.</li> <li>Contributing Guide: The <code>CONTRIBUTION/CONTRIBUTING.md</code> file is in a dedicated <code>CONTRIBUTION</code> folder.</li> <li>Security Policy: The <code>SECURITY.md</code> file is also often found at the top level or within the <code>.github/</code> folder.</li> <li>Code Owners: The <code>CODEOWNERS.md</code> file (in <code>.github/</code>) automatically requests reviews from specific people when certain parts of the code are changed.</li> </ol> <p>Here's a simplified diagram showing where these files are within the GitHub repository:</p> <pre><code>graph TD\n    A[ShinyOlink GitHub Repository] --&gt; B[.github/ folder]\n    A --&gt; C[CONTRIBUTION/ folder]\n    A --&gt; D[CODE_OF_CONDUCT.md]\n    A --&gt; E[SECURITY.md]\n    B --&gt; F[ISSUE_TEMPLATE/ folder]\n    B --&gt; G[CODEOWNERS.md]\n    F --&gt; H[bug_report.md]\n    F --&gt; I[feature_request.md]\n    C --&gt; J[CONTRIBUTING.md]</code></pre>"},{"location":"chapter8/#diving-into-the-code-the-files-themselves","title":"Diving into the Code: The Files Themselves","text":"<p>While we've seen snippets above, it's helpful to know these are just plain text files written in Markdown. GitHub then renders them nicely on the website.</p> <p>For example, the <code>CODEOWNERS.md</code> file helps automate the review process for contributions:</p> <p><pre><code># CODEOWNERS.md (Simplified)\n# https://help.github.com/articles/about-codeowners/\n\n# These owners will be the default owners for everything in\n# the repo. Unless a later match takes precedence, they will\n# be requested for review when someone opens a PR.\n*       @JD2112\n</code></pre> Explanation: This small file tells GitHub that <code>@JD2112</code> (likely the main maintainer or a team) should be asked to review any changes (Pull Requests) that affect the project. This ensures that important changes are always looked at by someone responsible.</p> <p>These documents are the bedrock of <code>ShinyOlink</code>'s community and development process. They formalize how the project functions, making it more predictable, fair, and welcoming for everyone involved.</p>"},{"location":"chapter8/#conclusion","title":"Conclusion","text":"<p>In this final chapter, you've learned about the vital concept of Project Governance &amp; Contributions in <code>ShinyOlink</code>:</p> <ul> <li>It's the \"rulebook\" and \"customer service\" department that ensures the project's health, quality, and community interactions.</li> <li>You now understand the purpose and location of key documents like Issue Templates, the Contributing Guide, the Code of Conduct, and the Security Policy.</li> <li>You've seen how these documents guide you in reporting bugs, requesting features, contributing code, and behaving respectfully within the <code>ShinyOlink</code> community.</li> </ul> <p>By following these guidelines, you can effectively interact with the <code>ShinyOlink</code> project, whether you're a user providing feedback or a developer contributing code. This structured approach helps ensure <code>ShinyOlink</code> continues to grow and improve as a valuable tool for Olink proteomics analysis, thanks to the collective efforts of its community!</p>"}]}